# -*- coding: utf-8 -*-
"""Inference_eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rc27DpCA-jW-4ZJRw_-EOKDkae6zM1wp
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q dotenv

import torch
from datasets import load_dataset
from transformers import (AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, pipeline, logging,)
from datasets import load_dataset
import os
import json
from pathlib import Path

env_path = '/content/drive/MyDrive/Colab_Notebooks/'

import os
from dotenv import load_dotenv

env_path = os.path.join(env_path, ".env")

load_dotenv(env_path)

# Access your keys
hf_token = os.getenv("HF_TOKEN")

#!pip install -U datasets

ds = load_dataset("b-mc2/sql-create-context")
test_ds = ds['train'].shuffle(seed=36).select(range(10))

def transform_eval(element):
  input = element['question']
  context = element['context']
  actual_response = element['answer']
  return {"prompt" : f'''You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.

You must output the SQL query that answers the question.

### Input:
{input}

### Context:
{context}''' ,
          "actual_response" : actual_response}


test_transformed = test_ds.map(transform_eval)

#test_transformed.push_to_hub("sql-context-llama2-10-eval")

#!pip install -U bitsandbytes

#device_map = {"": 0}
device_map = "auto"
model_name = "openlm-research/open_llama_7b_v2"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    #quantization_config=bnb_config,
    load_in_8bit=True,
    torch_dtype=torch.bfloat16,
    device_map=device_map
    )

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

model_new_name = "kundan05/Llama-2-7b-sql-chat-finetuned-1k"

model_new_sql = AutoModelForCausalLM.from_pretrained(
    model_new_name,
    #quantization_config=bnb_config,
    load_in_8bit=True,
    torch_dtype=torch.bfloat16,
    device_map=device_map
    )

tokenizer_new = AutoTokenizer.from_pretrained(model_new_name, trust_remote_code=True)
tokenizer_new.pad_token = tokenizer.eos_token
tokenizer_new.padding_side = "right"

def eval(element):
  prompt = element['prompt']
  actual_response = element['actual_response']

  pipe_base = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=700)
  result_base = pipe_base(prompt)
  response_base = result_base[0]['generated_text']

  pipe_new = pipeline(task="text-generation", model=model_new_sql, tokenizer=tokenizer_new, max_length=700)
  result_new = pipe_new(prompt)
  response_finetuned = result_new[0]['generated_text']

  return {"prompt" : prompt,
          "actual_response" : actual_response,
          "response_base" : response_base,
          "response_finetuned" : response_finetuned}

lst = []
for element in test_transformed:
  lst.append(eval(element))

lst[0]

import pandas as pd
import re

# Example list of dictionaries (add all 10 like this)
data = lst

# Extract relevant parts
records = []

for entry in data:
    prompt = entry['prompt']

    # Extract Input and Context from the prompt
    input_match = re.search(r"### Input:\n(.*?)\n\n### Context:", prompt, re.DOTALL)
    context_match = re.search(r"### Context:\n(.*)", prompt, re.DOTALL)

    input_text = input_match.group(1).strip() if input_match else ''
    context_text = context_match.group(1).strip() if context_match else ''

    # Extract responses
    actual_response = entry.get('actual_response', '').strip()

    base_response_match = re.search(r"### Output:\n(.*)", entry.get('response_base', ''), re.DOTALL)
    finetuned_response_match = re.search(r"### Response:\n(.*)", entry.get('response_finetuned', ''), re.DOTALL)

    base_response = base_response_match.group(1).strip() if base_response_match else ''
    finetuned_response = finetuned_response_match.group(1).strip() if finetuned_response_match else ''

    records.append({
        "Input": input_text,
        "Context": context_text,
        "Actual Response": actual_response,
        "Base Response": base_response,
        "Finetuned Response": finetuned_response
    })

# Convert to DataFrame
df = pd.DataFrame(records)

# Show it
df.head()

df.to_csv('final_eval_finetune.csv')